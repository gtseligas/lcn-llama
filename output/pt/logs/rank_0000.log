
[2025-02-12 22:07:59,532][oumi][rank0][pid:1820258][MainThread][INFO]][train.py:158] Resolved 'training.dataloader_num_workers=auto' to 'training.dataloader_num_workers=2'
[2025-02-12 22:07:59,533][oumi][rank0][pid:1820258][MainThread][INFO]][train.py:188] TrainingConfig:
TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='cerebras/SlimPajama-627B',
                                                                                dataset_path=None,
                                                                                subset=None,
                                                                                split='train',
                                                                                dataset_kwargs={'seq_length': 2048},
                                                                                sample_count=None,
                                                                                mixture_proportion=None,
                                                                                shuffle=False,
                                                                                seed=None,
                                                                                shuffle_buffer_size=1000,
                                                                                trust_remote_code=False,
                                                                                transform_num_workers=None)],
                                                        collator_name=None,
                                                        pack=True,
                                                        stream=True,
                                                        target_col='text',
                                                        mixture_strategy='first_exhausted',
                                                        seed=None,
                                                        use_async_dataset=True,
                                                        use_torchdata=None),
                               test=DatasetSplitParams(datasets=[],
                                                       collator_name=None,
                                                       pack=False,
                                                       stream=False,
                                                       target_col=None,
                                                       mixture_strategy='first_exhausted',
                                                       seed=None,
                                                       use_async_dataset=False,
                                                       use_torchdata=None),
                               validation=DatasetSplitParams(datasets=[],
                                                             collator_name=None,
                                                             pack=False,
                                                             stream=False,
                                                             target_col=None,
                                                             mixture_strategy='first_exhausted',
                                                             seed=None,
                                                             use_async_dataset=False,
                                                             use_torchdata=None)),
               model=ModelParams(model_name='meta-llama/Llama-3.2-1B',
                                 adapter_model=None,
                                 tokenizer_name=None,
                                 tokenizer_pad_token=None,
                                 tokenizer_kwargs={},
                                 model_max_length=2048,
                                 load_pretrained_weights=False,
                                 trust_remote_code=True,
                                 torch_dtype_str='bfloat16',
                                 compile=False,
                                 chat_template='chat_ml',
                                 attn_implementation='sdpa',
                                 device_map='auto',
                                 model_kwargs={},
                                 enable_liger_kernel=True,
                                 shard_for_eval=False,
                                 freeze_layers=[]),
               training=TrainingParams(use_peft=False,
                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,
                                       enable_gradient_checkpointing=True,
                                       gradient_checkpointing_kwargs={'use_reentrant': False},
                                       output_dir='output/pt',
                                       per_device_train_batch_size=16,
                                       per_device_eval_batch_size=8,
                                       gradient_accumulation_steps=1,
                                       max_steps=1000,
                                       num_train_epochs=3,
                                       save_epoch=False,
                                       save_steps=500,
                                       save_final_model=True,
                                       seed=123,
                                       run_name=None,
                                       metrics_function=None,
                                       log_level='info',
                                       dep_log_level='warning',
                                       enable_wandb=False,
                                       enable_tensorboard=True,
                                       logging_strategy='steps',
                                       logging_dir=None,
                                       logging_steps=10,
                                       logging_first_step=False,
                                       eval_strategy='no',
                                       eval_steps=500,
                                       learning_rate=5e-05,
                                       lr_scheduler_type='linear',
                                       lr_scheduler_kwargs={},
                                       warmup_ratio=None,
                                       warmup_steps=None,
                                       optimizer='adamw_torch_fused',
                                       weight_decay=0.0,
                                       adam_beta1=0.9,
                                       adam_beta2=0.999,
                                       adam_epsilon=1e-08,
                                       sgd_momentum=0.0,
                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,
                                       compile=True,
                                       include_performance_metrics=True,
                                       include_alternative_mfu_metrics=False,
                                       log_model_summary=False,
                                       resume_from_checkpoint=None,
                                       try_resume_from_last_checkpoint=False,
                                       dataloader_num_workers=2,
                                       dataloader_prefetch_factor=32,
                                       dataloader_main_process_only=False,
                                       ddp_find_unused_parameters=False,
                                       max_grad_norm=1.0,
                                       trainer_kwargs={'max_seq_length': 2048},
                                       profiler=ProfilerParams(save_dir=None,
                                                               enable_cpu_profiling=False,
                                                               enable_cuda_profiling=False,
                                                               record_shapes=False,
                                                               profile_memory=False,
                                                               with_stack=False,
                                                               with_flops=False,
                                                               with_modules=False,
                                                               row_limit=50,
                                                               schedule=ProfilerScheduleParams(enable_schedule=False,
                                                                                               wait=0,
                                                                                               warmup=1,
                                                                                               active=3,
                                                                                               repeat=1,
                                                                                               skip_first=1)),
                                       telemetry=TelemetryParams(telemetry_dir='telemetry',
                                                                 collect_telemetry_for_all_ranks=False,
                                                                 track_gpu_temperature=False),
                                       empty_device_cache_steps=1,
                                       nccl_default_timeout_minutes=None),
               peft=PeftParams(lora_r=8,
                               lora_alpha=8,
                               lora_dropout=0.0,
                               lora_target_modules=None,
                               lora_modules_to_save=None,
                               lora_bias='none',
                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,
                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,
                               q_lora=False,
                               q_lora_bits=4,
                               bnb_4bit_quant_type='fp4',
                               use_bnb_nested_quant=False,
                               bnb_4bit_quant_storage='uint8',
                               bnb_4bit_compute_dtype='float32',
                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),
               fsdp=FSDPParams(enable_fsdp=True,
                               sharding_strategy=<ShardingStrategy.HYBRID_SHARD: 'HYBRID_SHARD'>,
                               cpu_offload=False,
                               mixed_precision=None,
                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,
                               forward_prefetch=True,
                               use_orig_params=None,
                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,
                               auto_wrap_policy=<AutoWrapPolicy.TRANSFORMER_BASED_WRAP: 'TRANSFORMER_BASED_WRAP'>,
                               min_num_params=100000,
                               transformer_layer_cls='LlamaDecoderLayer',
                               sync_module_states=True))
[2025-02-12 22:07:59,562][oumi][rank0][pid:1820258][MainThread][INFO]][train.py:206] Set Accelerate environment variables for FSDP: {'ACCELERATE_DYNAMO_BACKEND': 'NO', 'ACCELERATE_DYNAMO_MODE': 'default', 'ACCELERATE_DYNAMO_USE_FULLGRAPH': 'False', 'ACCELERATE_DYNAMO_USE_DYNAMIC': 'False', 'FSDP_CPU_RAM_EFFICIENT_LOADING': 'true', 'FSDP_USE_ORIG_PARAMS': 'true', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_SHARDING_STRATEGY': 'HYBRID_SHARD', 'FSDP_OFFLOAD_PARAMS': 'false', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'true', 'FSDP_STATE_DICT_TYPE': 'FULL_STATE_DICT', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_MIN_NUM_PARAMS': '100000', 'FSDP_TRANSFORMER_CLS_TO_WRAP': 'LlamaDecoderLayer', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_ACTIVATION_CHECKPOINTING': 'true'}
[2025-02-12 22:08:00,349][oumi][rank0][pid:1820258][MainThread][WARNING]][models.py:439] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.
[2025-02-12 22:08:00,350][oumi][rank0][pid:1820258][MainThread][INFO]][models.py:455] Using the chat template 'chat_ml' specified in model config for model 'meta-llama/Llama-3.2-1B'. 
[2025-02-12 22:08:00,369][oumi][rank0][pid:1820258][MainThread][INFO]][models.py:199] Accelerate FSDP run detected! Setting device_map to None.
[2025-02-12 22:08:00,369][oumi][rank0][pid:1820258][MainThread][INFO]][models.py:208] Building model using device_map: None (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...
[2025-02-12 22:08:00,369][oumi][rank0][pid:1820258][MainThread][INFO]][models.py:276] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.
[2025-02-12 22:09:11,756][oumi][rank0][pid:1820258][MainThread][INFO]][base_iterable_dataset.py:47] Creating iterable dataset (type: SlimPajamaDataset)...
[2025-02-12 22:22:00,243][oumi][rank0][pid:1820258][MainThread][INFO]][torch_profiler_utils.py:164] PROF: Torch Profiler disabled!
[2025-02-12 22:22:00,298][oumi][rank0][pid:1820258][MainThread][WARNING]][callbacks.py:62] MFU logging is only supported on GPU. Skipping MFU callbacks.
[2025-02-12 22:22:00,315][oumi][rank0][pid:1820258][MainThread][INFO]][training.py:63] SFTConfig(output_dir='output/pt',
          overwrite_output_dir=False,
          do_train=False,
          do_eval=False,
          do_predict=False,
          eval_strategy=<IntervalStrategy.NO: 'no'>,
          prediction_loss_only=False,
          per_device_train_batch_size=16,
          per_device_eval_batch_size=8,
          per_gpu_train_batch_size=None,
          per_gpu_eval_batch_size=None,
          gradient_accumulation_steps=1,
          eval_accumulation_steps=None,
          eval_delay=0,
          torch_empty_cache_steps=1,
          learning_rate=5e-05,
          weight_decay=0.0,
          adam_beta1=0.9,
          adam_beta2=0.999,
          adam_epsilon=1e-08,
          max_grad_norm=1.0,
          num_train_epochs=3,
          max_steps=1000,
          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,
          lr_scheduler_kwargs={},
          warmup_ratio=0.0,
          warmup_steps=0,
          log_level='warning',
          log_level_replica='warning',
          log_on_each_node=True,
          logging_dir='output/pt/runs/Feb12_22-22-00_lrdn3456.leonardo.local',
          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,
          logging_first_step=False,
          logging_steps=10,
          logging_nan_inf_filter=True,
          save_strategy=<SaveStrategy.STEPS: 'steps'>,
          save_steps=500,
          save_total_limit=None,
          save_safetensors=True,
          save_on_each_node=False,
          save_only_model=False,
          restore_callback_states_from_checkpoint=False,
          no_cuda=False,
          use_cpu=False,
          use_mps_device=False,
          seed=123,
          data_seed=None,
          jit_mode_eval=False,
          use_ipex=False,
          bf16=False,
          fp16=False,
          fp16_opt_level='O1',
          half_precision_backend='auto',
          bf16_full_eval=False,
          fp16_full_eval=False,
          tf32=None,
          local_rank=0,
          ddp_backend=None,
          tpu_num_cores=None,
          tpu_metrics_debug=False,
          debug=[],
          dataloader_drop_last=False,
          eval_steps=500,
          dataloader_num_workers=2,
          dataloader_prefetch_factor=32,
          past_index=-1,
          run_name='output/pt',
          disable_tqdm=False,
          remove_unused_columns=True,
          label_names=None,
          load_best_model_at_end=False,
          metric_for_best_model=None,
          greater_is_better=None,
          ignore_data_skip=False,
          fsdp=[],
          fsdp_min_num_params=0,
          fsdp_config={'min_num_params': 0,
                       'xla': False,
                       'xla_fsdp_grad_ckpt': False,
                       'xla_fsdp_v2': False},
          fsdp_transformer_layer_cls_to_wrap=None,
          accelerator_config=AcceleratorConfig(split_batches=False,
                                               dispatch_batches=False,
                                               even_batches=True,
                                               use_seedable_sampler=True,
                                               non_blocking=False,
                                               gradient_accumulation_kwargs=None,
                                               use_configured_state=False),
          deepspeed=None,
          label_smoothing_factor=0.0,
          optim=<OptimizerNames.ADAMW_TORCH_FUSED: 'adamw_torch_fused'>,
          optim_args=None,
          adafactor=False,
          group_by_length=False,
          length_column_name='length',
          report_to=['tensorboard'],
          ddp_find_unused_parameters=False,
          ddp_bucket_cap_mb=None,
          ddp_broadcast_buffers=None,
          dataloader_pin_memory=True,
          dataloader_persistent_workers=False,
          skip_memory_metrics=True,
          use_legacy_prediction_loop=False,
          push_to_hub=False,
          resume_from_checkpoint=None,
          hub_model_id=None,
          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,
          hub_token=None,
          hub_private_repo=None,
          hub_always_push=False,
          gradient_checkpointing=False,
          gradient_checkpointing_kwargs={'use_reentrant': False},
          include_inputs_for_metrics=False,
          include_for_metrics=[],
          eval_do_concat_batches=True,
          fp16_backend='auto',
          evaluation_strategy=None,
          push_to_hub_model_id=None,
          push_to_hub_organization=None,
          push_to_hub_token=None,
          mp_parameters='',
          auto_find_batch_size=False,
          full_determinism=False,
          torchdynamo=None,
          ray_scope='last',
          ddp_timeout=1800,
          torch_compile=True,
          torch_compile_backend='inductor',
          torch_compile_mode=None,
          dispatch_batches=False,
          split_batches=None,
          include_tokens_per_second=True,
          include_num_input_tokens_seen=True,
          neftune_noise_alpha=None,
          optim_target_modules=None,
          batch_eval_metrics=False,
          eval_on_start=False,
          use_liger_kernel=False,
          eval_use_gather_object=False,
          average_tokens_across_devices=False,
          dataset_text_field='text',
          packing=False,
          max_seq_length=2048,
          dataset_num_proc=None,
          dataset_batch_size=1000,
          model_init_kwargs=None,
          dataset_kwargs=None,
          eval_packing=None,
          num_of_sequences=1024,
          chars_per_token=3.6,
          use_liger=False)
[2025-02-13 14:21:39,350][oumi][rank0][pid:529184][MainThread][INFO]][train.py:158] Resolved 'training.dataloader_num_workers=auto' to 'training.dataloader_num_workers=2'
[2025-02-13 14:21:39,352][oumi][rank0][pid:529184][MainThread][INFO]][train.py:188] TrainingConfig:
TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='cerebras/SlimPajama-627B',
                                                                                dataset_path=None,
                                                                                subset=None,
                                                                                split='train',
                                                                                dataset_kwargs={'seq_length': 2048},
                                                                                sample_count=None,
                                                                                mixture_proportion=None,
                                                                                shuffle=False,
                                                                                seed=None,
                                                                                shuffle_buffer_size=1000,
                                                                                trust_remote_code=False,
                                                                                transform_num_workers=None)],
                                                        collator_name=None,
                                                        pack=True,
                                                        stream=True,
                                                        target_col='text',
                                                        mixture_strategy='first_exhausted',
                                                        seed=None,
                                                        use_async_dataset=True,
                                                        use_torchdata=None),
                               test=DatasetSplitParams(datasets=[],
                                                       collator_name=None,
                                                       pack=False,
                                                       stream=False,
                                                       target_col=None,
                                                       mixture_strategy='first_exhausted',
                                                       seed=None,
                                                       use_async_dataset=False,
                                                       use_torchdata=None),
                               validation=DatasetSplitParams(datasets=[],
                                                             collator_name=None,
                                                             pack=False,
                                                             stream=False,
                                                             target_col=None,
                                                             mixture_strategy='first_exhausted',
                                                             seed=None,
                                                             use_async_dataset=False,
                                                             use_torchdata=None)),
               model=ModelParams(model_name='meta-llama/Llama-3.2-1B',
                                 adapter_model=None,
                                 tokenizer_name=None,
                                 tokenizer_pad_token=None,
                                 tokenizer_kwargs={},
                                 model_max_length=2048,
                                 load_pretrained_weights=False,
                                 trust_remote_code=True,
                                 torch_dtype_str='bfloat16',
                                 compile=False,
                                 chat_template='chat_ml',
                                 attn_implementation='sdpa',
                                 device_map='auto',
                                 model_kwargs={},
                                 enable_liger_kernel=True,
                                 shard_for_eval=False,
                                 freeze_layers=[]),
               training=TrainingParams(use_peft=False,
                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,
                                       enable_gradient_checkpointing=True,
                                       gradient_checkpointing_kwargs={'use_reentrant': False},
                                       output_dir='output/pt',
                                       per_device_train_batch_size=16,
                                       per_device_eval_batch_size=8,
                                       gradient_accumulation_steps=1,
                                       max_steps=1000,
                                       num_train_epochs=3,
                                       save_epoch=False,
                                       save_steps=500,
                                       save_final_model=True,
                                       seed=123,
                                       run_name=None,
                                       metrics_function=None,
                                       log_level='info',
                                       dep_log_level='warning',
                                       enable_wandb=False,
                                       enable_tensorboard=True,
                                       logging_strategy='steps',
                                       logging_dir=None,
                                       logging_steps=10,
                                       logging_first_step=False,
                                       eval_strategy='no',
                                       eval_steps=500,
                                       learning_rate=5e-05,
                                       lr_scheduler_type='linear',
                                       lr_scheduler_kwargs={},
                                       warmup_ratio=None,
                                       warmup_steps=None,
                                       optimizer='adamw_torch_fused',
                                       weight_decay=0.0,
                                       adam_beta1=0.9,
                                       adam_beta2=0.999,
                                       adam_epsilon=1e-08,
                                       sgd_momentum=0.0,
                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,
                                       compile=True,
                                       include_performance_metrics=True,
                                       include_alternative_mfu_metrics=False,
                                       log_model_summary=False,
                                       resume_from_checkpoint=None,
                                       try_resume_from_last_checkpoint=False,
                                       dataloader_num_workers=2,
                                       dataloader_prefetch_factor=32,
                                       dataloader_main_process_only=False,
                                       ddp_find_unused_parameters=False,
                                       max_grad_norm=1.0,
                                       trainer_kwargs={'max_seq_length': 2048},
                                       profiler=ProfilerParams(save_dir=None,
                                                               enable_cpu_profiling=False,
                                                               enable_cuda_profiling=False,
                                                               record_shapes=False,
                                                               profile_memory=False,
                                                               with_stack=False,
                                                               with_flops=False,
                                                               with_modules=False,
                                                               row_limit=50,
                                                               schedule=ProfilerScheduleParams(enable_schedule=False,
                                                                                               wait=0,
                                                                                               warmup=1,
                                                                                               active=3,
                                                                                               repeat=1,
                                                                                               skip_first=1)),
                                       telemetry=TelemetryParams(telemetry_dir='telemetry',
                                                                 collect_telemetry_for_all_ranks=False,
                                                                 track_gpu_temperature=False),
                                       empty_device_cache_steps=1,
                                       nccl_default_timeout_minutes=None),
               peft=PeftParams(lora_r=8,
                               lora_alpha=8,
                               lora_dropout=0.0,
                               lora_target_modules=None,
                               lora_modules_to_save=None,
                               lora_bias='none',
                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,
                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,
                               q_lora=False,
                               q_lora_bits=4,
                               bnb_4bit_quant_type='fp4',
                               use_bnb_nested_quant=False,
                               bnb_4bit_quant_storage='uint8',
                               bnb_4bit_compute_dtype='float32',
                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),
               fsdp=FSDPParams(enable_fsdp=True,
                               sharding_strategy=<ShardingStrategy.HYBRID_SHARD: 'HYBRID_SHARD'>,
                               cpu_offload=False,
                               mixed_precision=None,
                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,
                               forward_prefetch=True,
                               use_orig_params=None,
                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,
                               auto_wrap_policy=<AutoWrapPolicy.TRANSFORMER_BASED_WRAP: 'TRANSFORMER_BASED_WRAP'>,
                               min_num_params=100000,
                               transformer_layer_cls='LlamaDecoderLayer',
                               sync_module_states=True))
[2025-02-13 14:21:39,380][oumi][rank0][pid:529184][MainThread][INFO]][train.py:206] Set Accelerate environment variables for FSDP: {'ACCELERATE_DYNAMO_BACKEND': 'NO', 'ACCELERATE_DYNAMO_MODE': 'default', 'ACCELERATE_DYNAMO_USE_FULLGRAPH': 'False', 'ACCELERATE_DYNAMO_USE_DYNAMIC': 'False', 'FSDP_CPU_RAM_EFFICIENT_LOADING': 'true', 'FSDP_USE_ORIG_PARAMS': 'true', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_SHARDING_STRATEGY': 'HYBRID_SHARD', 'FSDP_OFFLOAD_PARAMS': 'false', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'true', 'FSDP_STATE_DICT_TYPE': 'FULL_STATE_DICT', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_MIN_NUM_PARAMS': '100000', 'FSDP_TRANSFORMER_CLS_TO_WRAP': 'LlamaDecoderLayer', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_ACTIVATION_CHECKPOINTING': 'true'}
[2025-02-13 14:21:40,243][oumi][rank0][pid:529184][MainThread][WARNING]][models.py:439] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.
[2025-02-13 14:21:40,246][oumi][rank0][pid:529184][MainThread][INFO]][models.py:455] Using the chat template 'chat_ml' specified in model config for model 'meta-llama/Llama-3.2-1B'. 
[2025-02-13 14:21:40,258][oumi][rank0][pid:529184][MainThread][INFO]][models.py:199] Accelerate FSDP run detected! Setting device_map to None.
[2025-02-13 14:21:40,258][oumi][rank0][pid:529184][MainThread][INFO]][models.py:208] Building model using device_map: None (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...
[2025-02-13 14:21:40,258][oumi][rank0][pid:529184][MainThread][INFO]][models.py:276] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.
[2025-02-13 14:22:40,305][oumi][rank0][pid:529184][MainThread][INFO]][base_iterable_dataset.py:47] Creating iterable dataset (type: SlimPajamaDataset)...
[2025-02-13 14:33:00,045][oumi][rank0][pid:2800716][MainThread][INFO]][train.py:158] Resolved 'training.dataloader_num_workers=auto' to 'training.dataloader_num_workers=2'
[2025-02-13 14:33:00,047][oumi][rank0][pid:2800716][MainThread][INFO]][train.py:188] TrainingConfig:
TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='cerebras/SlimPajama-627B',
                                                                                dataset_path=None,
                                                                                subset=None,
                                                                                split='train',
                                                                                dataset_kwargs={'seq_length': 2048},
                                                                                sample_count=None,
                                                                                mixture_proportion=None,
                                                                                shuffle=False,
                                                                                seed=None,
                                                                                shuffle_buffer_size=1000,
                                                                                trust_remote_code=False,
                                                                                transform_num_workers=None)],
                                                        collator_name=None,
                                                        pack=True,
                                                        stream=True,
                                                        target_col='text',
                                                        mixture_strategy='first_exhausted',
                                                        seed=None,
                                                        use_async_dataset=True,
                                                        use_torchdata=None),
                               test=DatasetSplitParams(datasets=[],
                                                       collator_name=None,
                                                       pack=False,
                                                       stream=False,
                                                       target_col=None,
                                                       mixture_strategy='first_exhausted',
                                                       seed=None,
                                                       use_async_dataset=False,
                                                       use_torchdata=None),
                               validation=DatasetSplitParams(datasets=[],
                                                             collator_name=None,
                                                             pack=False,
                                                             stream=False,
                                                             target_col=None,
                                                             mixture_strategy='first_exhausted',
                                                             seed=None,
                                                             use_async_dataset=False,
                                                             use_torchdata=None)),
               model=ModelParams(model_name='meta-llama/Llama-3.2-1B',
                                 adapter_model=None,
                                 tokenizer_name=None,
                                 tokenizer_pad_token=None,
                                 tokenizer_kwargs={},
                                 model_max_length=2048,
                                 load_pretrained_weights=False,
                                 trust_remote_code=True,
                                 torch_dtype_str='bfloat16',
                                 compile=False,
                                 chat_template='chat_ml',
                                 attn_implementation='sdpa',
                                 device_map='auto',
                                 model_kwargs={},
                                 enable_liger_kernel=True,
                                 shard_for_eval=False,
                                 freeze_layers=[]),
               training=TrainingParams(use_peft=False,
                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,
                                       enable_gradient_checkpointing=True,
                                       gradient_checkpointing_kwargs={'use_reentrant': False},
                                       output_dir='output/pt',
                                       per_device_train_batch_size=16,
                                       per_device_eval_batch_size=8,
                                       gradient_accumulation_steps=1,
                                       max_steps=1000,
                                       num_train_epochs=3,
                                       save_epoch=False,
                                       save_steps=500,
                                       save_final_model=True,
                                       seed=123,
                                       run_name=None,
                                       metrics_function=None,
                                       log_level='info',
                                       dep_log_level='warning',
                                       enable_wandb=False,
                                       enable_tensorboard=True,
                                       logging_strategy='steps',
                                       logging_dir=None,
                                       logging_steps=10,
                                       logging_first_step=False,
                                       eval_strategy='no',
                                       eval_steps=500,
                                       learning_rate=5e-05,
                                       lr_scheduler_type='linear',
                                       lr_scheduler_kwargs={},
                                       warmup_ratio=None,
                                       warmup_steps=None,
                                       optimizer='adamw_torch_fused',
                                       weight_decay=0.0,
                                       adam_beta1=0.9,
                                       adam_beta2=0.999,
                                       adam_epsilon=1e-08,
                                       sgd_momentum=0.0,
                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,
                                       compile=True,
                                       include_performance_metrics=True,
                                       include_alternative_mfu_metrics=False,
                                       log_model_summary=False,
                                       resume_from_checkpoint=None,
                                       try_resume_from_last_checkpoint=False,
                                       dataloader_num_workers=2,
                                       dataloader_prefetch_factor=32,
                                       dataloader_main_process_only=False,
                                       ddp_find_unused_parameters=False,
                                       max_grad_norm=1.0,
                                       trainer_kwargs={'max_seq_length': 2048},
                                       profiler=ProfilerParams(save_dir=None,
                                                               enable_cpu_profiling=False,
                                                               enable_cuda_profiling=False,
                                                               record_shapes=False,
                                                               profile_memory=False,
                                                               with_stack=False,
                                                               with_flops=False,
                                                               with_modules=False,
                                                               row_limit=50,
                                                               schedule=ProfilerScheduleParams(enable_schedule=False,
                                                                                               wait=0,
                                                                                               warmup=1,
                                                                                               active=3,
                                                                                               repeat=1,
                                                                                               skip_first=1)),
                                       telemetry=TelemetryParams(telemetry_dir='telemetry',
                                                                 collect_telemetry_for_all_ranks=False,
                                                                 track_gpu_temperature=False),
                                       empty_device_cache_steps=1,
                                       nccl_default_timeout_minutes=None),
               peft=PeftParams(lora_r=8,
                               lora_alpha=8,
                               lora_dropout=0.0,
                               lora_target_modules=None,
                               lora_modules_to_save=None,
                               lora_bias='none',
                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,
                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,
                               q_lora=False,
                               q_lora_bits=4,
                               bnb_4bit_quant_type='fp4',
                               use_bnb_nested_quant=False,
                               bnb_4bit_quant_storage='uint8',
                               bnb_4bit_compute_dtype='float32',
                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),
               fsdp=FSDPParams(enable_fsdp=True,
                               sharding_strategy=<ShardingStrategy.HYBRID_SHARD: 'HYBRID_SHARD'>,
                               cpu_offload=False,
                               mixed_precision=None,
                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,
                               forward_prefetch=True,
                               use_orig_params=None,
                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,
                               auto_wrap_policy=<AutoWrapPolicy.TRANSFORMER_BASED_WRAP: 'TRANSFORMER_BASED_WRAP'>,
                               min_num_params=100000,
                               transformer_layer_cls='LlamaDecoderLayer',
                               sync_module_states=True))
[2025-02-13 14:33:00,075][oumi][rank0][pid:2800716][MainThread][INFO]][train.py:206] Set Accelerate environment variables for FSDP: {'ACCELERATE_DYNAMO_BACKEND': 'NO', 'ACCELERATE_DYNAMO_MODE': 'default', 'ACCELERATE_DYNAMO_USE_FULLGRAPH': 'False', 'ACCELERATE_DYNAMO_USE_DYNAMIC': 'False', 'FSDP_CPU_RAM_EFFICIENT_LOADING': 'true', 'FSDP_USE_ORIG_PARAMS': 'true', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_SHARDING_STRATEGY': 'HYBRID_SHARD', 'FSDP_OFFLOAD_PARAMS': 'false', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'true', 'FSDP_STATE_DICT_TYPE': 'FULL_STATE_DICT', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_MIN_NUM_PARAMS': '100000', 'FSDP_TRANSFORMER_CLS_TO_WRAP': 'LlamaDecoderLayer', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_ACTIVATION_CHECKPOINTING': 'true'}
[2025-02-13 14:33:00,904][oumi][rank0][pid:2800716][MainThread][WARNING]][models.py:439] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.
[2025-02-13 14:33:00,905][oumi][rank0][pid:2800716][MainThread][INFO]][models.py:455] Using the chat template 'chat_ml' specified in model config for model 'meta-llama/Llama-3.2-1B'. 
[2025-02-13 14:33:00,914][oumi][rank0][pid:2800716][MainThread][INFO]][models.py:199] Accelerate FSDP run detected! Setting device_map to None.
[2025-02-13 14:33:00,914][oumi][rank0][pid:2800716][MainThread][INFO]][models.py:208] Building model using device_map: None (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...
[2025-02-13 14:33:00,914][oumi][rank0][pid:2800716][MainThread][INFO]][models.py:276] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.
[2025-02-13 14:34:00,932][oumi][rank0][pid:2800716][MainThread][INFO]][base_iterable_dataset.py:47] Creating iterable dataset (type: SlimPajamaDataset)...
[2025-02-13 14:44:45,144][oumi][rank0][pid:2800716][MainThread][INFO]][torch_profiler_utils.py:164] PROF: Torch Profiler disabled!
[2025-02-13 14:44:45,200][oumi][rank0][pid:2800716][MainThread][INFO]][callbacks.py:79] Number of model parameters for MFU: 973,146,112
[2025-02-13 14:44:45,200][oumi][rank0][pid:2800716][MainThread][INFO]][mfu_callback.py:81] MFU number of devices: 1
[2025-02-13 14:44:45,211][oumi][rank0][pid:2800716][MainThread][INFO]][mfu_callback.py:87] MFU device name: NVIDIA A100-SXM-64GB
[2025-02-13 14:44:45,254][oumi][rank0][pid:2800716][MainThread][INFO]][training.py:63] SFTConfig(output_dir='output/pt',
          overwrite_output_dir=False,
          do_train=False,
          do_eval=False,
          do_predict=False,
          eval_strategy=<IntervalStrategy.NO: 'no'>,
          prediction_loss_only=False,
          per_device_train_batch_size=16,
          per_device_eval_batch_size=8,
          per_gpu_train_batch_size=None,
          per_gpu_eval_batch_size=None,
          gradient_accumulation_steps=1,
          eval_accumulation_steps=None,
          eval_delay=0,
          torch_empty_cache_steps=1,
          learning_rate=5e-05,
          weight_decay=0.0,
          adam_beta1=0.9,
          adam_beta2=0.999,
          adam_epsilon=1e-08,
          max_grad_norm=1.0,
          num_train_epochs=3,
          max_steps=1000,
          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,
          lr_scheduler_kwargs={},
          warmup_ratio=0.0,
          warmup_steps=0,
          log_level='warning',
          log_level_replica='warning',
          log_on_each_node=True,
          logging_dir='output/pt/runs/Feb13_14-44-45_lrdn3450.leonardo.local',
          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,
          logging_first_step=False,
          logging_steps=10,
          logging_nan_inf_filter=True,
          save_strategy=<SaveStrategy.STEPS: 'steps'>,
          save_steps=500,
          save_total_limit=None,
          save_safetensors=True,
          save_on_each_node=False,
          save_only_model=False,
          restore_callback_states_from_checkpoint=False,
          no_cuda=False,
          use_cpu=False,
          use_mps_device=False,
          seed=123,
          data_seed=None,
          jit_mode_eval=False,
          use_ipex=False,
          bf16=False,
          fp16=False,
          fp16_opt_level='O1',
          half_precision_backend='auto',
          bf16_full_eval=False,
          fp16_full_eval=False,
          tf32=None,
          local_rank=0,
          ddp_backend=None,
          tpu_num_cores=None,
          tpu_metrics_debug=False,
          debug=[],
          dataloader_drop_last=False,
          eval_steps=500,
          dataloader_num_workers=2,
          dataloader_prefetch_factor=32,
          past_index=-1,
          run_name='output/pt',
          disable_tqdm=False,
          remove_unused_columns=True,
          label_names=None,
          load_best_model_at_end=False,
          metric_for_best_model=None,
          greater_is_better=None,
          ignore_data_skip=False,
          fsdp=[],
          fsdp_min_num_params=0,
          fsdp_config={'min_num_params': 0,
                       'xla': False,
                       'xla_fsdp_grad_ckpt': False,
                       'xla_fsdp_v2': False},
          fsdp_transformer_layer_cls_to_wrap=None,
          accelerator_config=AcceleratorConfig(split_batches=False,
                                               dispatch_batches=False,
                                               even_batches=True,
                                               use_seedable_sampler=True,
                                               non_blocking=False,
                                               gradient_accumulation_kwargs=None,
                                               use_configured_state=False),
          deepspeed=None,
          label_smoothing_factor=0.0,
          optim=<OptimizerNames.ADAMW_TORCH_FUSED: 'adamw_torch_fused'>,
          optim_args=None,
          adafactor=False,
          group_by_length=False,
          length_column_name='length',
          report_to=['tensorboard'],
          ddp_find_unused_parameters=False,
          ddp_bucket_cap_mb=None,
          ddp_broadcast_buffers=None,
          dataloader_pin_memory=True,
          dataloader_persistent_workers=False,
          skip_memory_metrics=True,
          use_legacy_prediction_loop=False,
          push_to_hub=False,
          resume_from_checkpoint=None,
          hub_model_id=None,
          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,
          hub_token=None,
          hub_private_repo=None,
          hub_always_push=False,
          gradient_checkpointing=False,
          gradient_checkpointing_kwargs={'use_reentrant': False},
          include_inputs_for_metrics=False,
          include_for_metrics=[],
          eval_do_concat_batches=True,
          fp16_backend='auto',
          evaluation_strategy=None,
          push_to_hub_model_id=None,
          push_to_hub_organization=None,
          push_to_hub_token=None,
          mp_parameters='',
          auto_find_batch_size=False,
          full_determinism=False,
          torchdynamo=None,
          ray_scope='last',
          ddp_timeout=1800,
          torch_compile=True,
          torch_compile_backend='inductor',
          torch_compile_mode=None,
          dispatch_batches=False,
          split_batches=None,
          include_tokens_per_second=True,
          include_num_input_tokens_seen=True,
          neftune_noise_alpha=None,
          optim_target_modules=None,
          batch_eval_metrics=False,
          eval_on_start=False,
          use_liger_kernel=False,
          eval_use_gather_object=False,
          average_tokens_across_devices=False,
          dataset_text_field='text',
          packing=False,
          max_seq_length=2048,
          dataset_num_proc=None,
          dataset_batch_size=1000,
          model_init_kwargs=None,
          dataset_kwargs=None,
          eval_packing=None,
          num_of_sequences=1024,
          chars_per_token=3.6,
          use_liger=False)
[2025-02-13 14:44:46,055][oumi][rank0][pid:2800716][MainThread][INFO]][device_utils.py:297] GPU Metrics Before Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=1, used_memory_mb=4034.0, temperature=44, fan_speed=None, fan_speeds=None, power_usage_watts=77.673, power_limit_watts=500.0, gpu_utilization=14, memory_utilization=0, performance_state=0, clock_speed_graphics=1245, clock_speed_sm=1245, clock_speed_memory=1593).
[2025-02-13 14:44:46,055][oumi][rank0][pid:2800716][MainThread][INFO]][train.py:326] Training init time: 706.356s
[2025-02-13 14:44:46,055][oumi][rank0][pid:2800716][MainThread][INFO]][train.py:327] Starting training... (TrainerType.TRL_SFT, transformers: 4.48.3)
[2025-02-13 15:01:09,984][oumi][rank0][pid:1122761][MainThread][INFO]][train.py:158] Resolved 'training.dataloader_num_workers=auto' to 'training.dataloader_num_workers=2'
[2025-02-13 15:01:09,987][oumi][rank0][pid:1122761][MainThread][INFO]][train.py:188] TrainingConfig:
TrainingConfig(data=DataParams(train=DatasetSplitParams(datasets=[DatasetParams(dataset_name='cerebras/SlimPajama-627B',
                                                                                dataset_path=None,
                                                                                subset=None,
                                                                                split='train',
                                                                                dataset_kwargs={'seq_length': 2048},
                                                                                sample_count=None,
                                                                                mixture_proportion=None,
                                                                                shuffle=False,
                                                                                seed=None,
                                                                                shuffle_buffer_size=1000,
                                                                                trust_remote_code=False,
                                                                                transform_num_workers=None)],
                                                        collator_name=None,
                                                        pack=True,
                                                        stream=True,
                                                        target_col='text',
                                                        mixture_strategy='first_exhausted',
                                                        seed=None,
                                                        use_async_dataset=True,
                                                        use_torchdata=None),
                               test=DatasetSplitParams(datasets=[],
                                                       collator_name=None,
                                                       pack=False,
                                                       stream=False,
                                                       target_col=None,
                                                       mixture_strategy='first_exhausted',
                                                       seed=None,
                                                       use_async_dataset=False,
                                                       use_torchdata=None),
                               validation=DatasetSplitParams(datasets=[],
                                                             collator_name=None,
                                                             pack=False,
                                                             stream=False,
                                                             target_col=None,
                                                             mixture_strategy='first_exhausted',
                                                             seed=None,
                                                             use_async_dataset=False,
                                                             use_torchdata=None)),
               model=ModelParams(model_name='meta-llama/Llama-3.2-1B',
                                 adapter_model=None,
                                 tokenizer_name=None,
                                 tokenizer_pad_token=None,
                                 tokenizer_kwargs={},
                                 model_max_length=2048,
                                 load_pretrained_weights=False,
                                 trust_remote_code=True,
                                 torch_dtype_str='bfloat16',
                                 compile=False,
                                 chat_template='chat_ml',
                                 attn_implementation='sdpa',
                                 device_map='auto',
                                 model_kwargs={},
                                 enable_liger_kernel=True,
                                 shard_for_eval=False,
                                 freeze_layers=[]),
               training=TrainingParams(use_peft=False,
                                       trainer_type=<TrainerType.TRL_SFT: 'trl_sft'>,
                                       enable_gradient_checkpointing=True,
                                       gradient_checkpointing_kwargs={'use_reentrant': False},
                                       output_dir='output/pt',
                                       per_device_train_batch_size=16,
                                       per_device_eval_batch_size=8,
                                       gradient_accumulation_steps=1,
                                       max_steps=1000,
                                       num_train_epochs=3,
                                       save_epoch=False,
                                       save_steps=500,
                                       save_final_model=True,
                                       seed=123,
                                       run_name=None,
                                       metrics_function=None,
                                       log_level='info',
                                       dep_log_level='warning',
                                       enable_wandb=False,
                                       enable_tensorboard=True,
                                       logging_strategy='steps',
                                       logging_dir=None,
                                       logging_steps=10,
                                       logging_first_step=False,
                                       eval_strategy='no',
                                       eval_steps=500,
                                       learning_rate=5e-05,
                                       lr_scheduler_type='linear',
                                       lr_scheduler_kwargs={},
                                       warmup_ratio=None,
                                       warmup_steps=None,
                                       optimizer='adamw_torch_fused',
                                       weight_decay=0.0,
                                       adam_beta1=0.9,
                                       adam_beta2=0.999,
                                       adam_epsilon=1e-08,
                                       sgd_momentum=0.0,
                                       mixed_precision_dtype=<MixedPrecisionDtype.NONE: 'none'>,
                                       compile=True,
                                       include_performance_metrics=True,
                                       include_alternative_mfu_metrics=False,
                                       log_model_summary=False,
                                       resume_from_checkpoint=None,
                                       try_resume_from_last_checkpoint=False,
                                       dataloader_num_workers=2,
                                       dataloader_prefetch_factor=32,
                                       dataloader_main_process_only=False,
                                       ddp_find_unused_parameters=False,
                                       max_grad_norm=1.0,
                                       trainer_kwargs={'max_seq_length': 2048},
                                       profiler=ProfilerParams(save_dir=None,
                                                               enable_cpu_profiling=False,
                                                               enable_cuda_profiling=False,
                                                               record_shapes=False,
                                                               profile_memory=False,
                                                               with_stack=False,
                                                               with_flops=False,
                                                               with_modules=False,
                                                               row_limit=50,
                                                               schedule=ProfilerScheduleParams(enable_schedule=False,
                                                                                               wait=0,
                                                                                               warmup=1,
                                                                                               active=3,
                                                                                               repeat=1,
                                                                                               skip_first=1)),
                                       telemetry=TelemetryParams(telemetry_dir='telemetry',
                                                                 collect_telemetry_for_all_ranks=False,
                                                                 track_gpu_temperature=False),
                                       empty_device_cache_steps=1,
                                       nccl_default_timeout_minutes=None),
               peft=PeftParams(lora_r=8,
                               lora_alpha=8,
                               lora_dropout=0.0,
                               lora_target_modules=None,
                               lora_modules_to_save=None,
                               lora_bias='none',
                               lora_init_weights=<LoraWeightInitialization.DEFAULT: 'default'>,
                               lora_task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>,
                               q_lora=False,
                               q_lora_bits=4,
                               bnb_4bit_quant_type='fp4',
                               use_bnb_nested_quant=False,
                               bnb_4bit_quant_storage='uint8',
                               bnb_4bit_compute_dtype='float32',
                               peft_save_mode=<PeftSaveMode.ADAPTER_ONLY: 'adapter_only'>),
               fsdp=FSDPParams(enable_fsdp=True,
                               sharding_strategy=<ShardingStrategy.HYBRID_SHARD: 'HYBRID_SHARD'>,
                               cpu_offload=False,
                               mixed_precision=None,
                               backward_prefetch=<BackwardPrefetch.BACKWARD_PRE: 'BACKWARD_PRE'>,
                               forward_prefetch=True,
                               use_orig_params=None,
                               state_dict_type=<StateDictType.FULL_STATE_DICT: 'FULL_STATE_DICT'>,
                               auto_wrap_policy=<AutoWrapPolicy.TRANSFORMER_BASED_WRAP: 'TRANSFORMER_BASED_WRAP'>,
                               min_num_params=100000,
                               transformer_layer_cls='LlamaDecoderLayer',
                               sync_module_states=True))
[2025-02-13 15:01:10,016][oumi][rank0][pid:1122761][MainThread][INFO]][train.py:206] Set Accelerate environment variables for FSDP: {'ACCELERATE_DYNAMO_BACKEND': 'NO', 'ACCELERATE_DYNAMO_MODE': 'default', 'ACCELERATE_DYNAMO_USE_FULLGRAPH': 'False', 'ACCELERATE_DYNAMO_USE_DYNAMIC': 'False', 'FSDP_CPU_RAM_EFFICIENT_LOADING': 'true', 'FSDP_USE_ORIG_PARAMS': 'true', 'ACCELERATE_USE_FSDP': 'true', 'FSDP_SHARDING_STRATEGY': 'HYBRID_SHARD', 'FSDP_OFFLOAD_PARAMS': 'false', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_FORWARD_PREFETCH': 'true', 'FSDP_STATE_DICT_TYPE': 'FULL_STATE_DICT', 'FSDP_AUTO_WRAP_POLICY': 'TRANSFORMER_BASED_WRAP', 'FSDP_MIN_NUM_PARAMS': '100000', 'FSDP_TRANSFORMER_CLS_TO_WRAP': 'LlamaDecoderLayer', 'FSDP_SYNC_MODULE_STATES': 'true', 'FSDP_ACTIVATION_CHECKPOINTING': 'true'}
[2025-02-13 15:01:10,873][oumi][rank0][pid:1122761][MainThread][WARNING]][models.py:439] Undefined pad token. Setting it to `<|finetune_right_pad_id|>`.
[2025-02-13 15:01:10,874][oumi][rank0][pid:1122761][MainThread][INFO]][models.py:455] Using the chat template 'chat_ml' specified in model config for model 'meta-llama/Llama-3.2-1B'. 
[2025-02-13 15:01:10,876][oumi][rank0][pid:1122761][MainThread][INFO]][models.py:199] Accelerate FSDP run detected! Setting device_map to None.
[2025-02-13 15:01:10,876][oumi][rank0][pid:1122761][MainThread][INFO]][models.py:208] Building model using device_map: None (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...
[2025-02-13 15:01:10,877][oumi][rank0][pid:1122761][MainThread][INFO]][models.py:276] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.
[2025-02-13 15:02:10,418][oumi][rank0][pid:1122761][MainThread][INFO]][base_iterable_dataset.py:47] Creating iterable dataset (type: SlimPajamaDataset)...
[2025-02-13 15:12:37,964][oumi][rank0][pid:1122761][MainThread][INFO]][torch_profiler_utils.py:164] PROF: Torch Profiler disabled!
[2025-02-13 15:12:37,999][oumi][rank0][pid:1122761][MainThread][INFO]][callbacks.py:79] Number of model parameters for MFU: 973,146,112
[2025-02-13 15:12:37,999][oumi][rank0][pid:1122761][MainThread][INFO]][mfu_callback.py:81] MFU number of devices: 1
[2025-02-13 15:12:37,999][oumi][rank0][pid:1122761][MainThread][INFO]][mfu_callback.py:87] MFU device name: NVIDIA A100-SXM-64GB
[2025-02-13 15:12:38,063][oumi][rank0][pid:1122761][MainThread][INFO]][training.py:63] SFTConfig(output_dir='output/pt',
          overwrite_output_dir=False,
          do_train=False,
          do_eval=False,
          do_predict=False,
          eval_strategy=<IntervalStrategy.NO: 'no'>,
          prediction_loss_only=False,
          per_device_train_batch_size=16,
          per_device_eval_batch_size=8,
          per_gpu_train_batch_size=None,
          per_gpu_eval_batch_size=None,
          gradient_accumulation_steps=1,
          eval_accumulation_steps=None,
          eval_delay=0,
          torch_empty_cache_steps=1,
          learning_rate=5e-05,
          weight_decay=0.0,
          adam_beta1=0.9,
          adam_beta2=0.999,
          adam_epsilon=1e-08,
          max_grad_norm=1.0,
          num_train_epochs=3,
          max_steps=1000,
          lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>,
          lr_scheduler_kwargs={},
          warmup_ratio=0.0,
          warmup_steps=0,
          log_level='warning',
          log_level_replica='warning',
          log_on_each_node=True,
          logging_dir='output/pt/runs/Feb13_15-12-38_lrdn2575.leonardo.local',
          logging_strategy=<IntervalStrategy.STEPS: 'steps'>,
          logging_first_step=False,
          logging_steps=10,
          logging_nan_inf_filter=True,
          save_strategy=<SaveStrategy.STEPS: 'steps'>,
          save_steps=500,
          save_total_limit=None,
          save_safetensors=True,
          save_on_each_node=False,
          save_only_model=False,
          restore_callback_states_from_checkpoint=False,
          no_cuda=False,
          use_cpu=False,
          use_mps_device=False,
          seed=123,
          data_seed=None,
          jit_mode_eval=False,
          use_ipex=False,
          bf16=False,
          fp16=False,
          fp16_opt_level='O1',
          half_precision_backend='auto',
          bf16_full_eval=False,
          fp16_full_eval=False,
          tf32=None,
          local_rank=0,
          ddp_backend=None,
          tpu_num_cores=None,
          tpu_metrics_debug=False,
          debug=[],
          dataloader_drop_last=False,
          eval_steps=500,
          dataloader_num_workers=2,
          dataloader_prefetch_factor=32,
          past_index=-1,
          run_name='output/pt',
          disable_tqdm=False,
          remove_unused_columns=True,
          label_names=None,
          load_best_model_at_end=False,
          metric_for_best_model=None,
          greater_is_better=None,
          ignore_data_skip=False,
          fsdp=[],
          fsdp_min_num_params=0,
          fsdp_config={'min_num_params': 0,
                       'xla': False,
                       'xla_fsdp_grad_ckpt': False,
                       'xla_fsdp_v2': False},
          fsdp_transformer_layer_cls_to_wrap=None,
          accelerator_config=AcceleratorConfig(split_batches=False,
                                               dispatch_batches=False,
                                               even_batches=True,
                                               use_seedable_sampler=True,
                                               non_blocking=False,
                                               gradient_accumulation_kwargs=None,
                                               use_configured_state=False),
          deepspeed=None,
          label_smoothing_factor=0.0,
          optim=<OptimizerNames.ADAMW_TORCH_FUSED: 'adamw_torch_fused'>,
          optim_args=None,
          adafactor=False,
          group_by_length=False,
          length_column_name='length',
          report_to=['tensorboard'],
          ddp_find_unused_parameters=False,
          ddp_bucket_cap_mb=None,
          ddp_broadcast_buffers=None,
          dataloader_pin_memory=True,
          dataloader_persistent_workers=False,
          skip_memory_metrics=True,
          use_legacy_prediction_loop=False,
          push_to_hub=False,
          resume_from_checkpoint=None,
          hub_model_id=None,
          hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>,
          hub_token=None,
          hub_private_repo=None,
          hub_always_push=False,
          gradient_checkpointing=False,
          gradient_checkpointing_kwargs={'use_reentrant': False},
          include_inputs_for_metrics=False,
          include_for_metrics=[],
          eval_do_concat_batches=True,
          fp16_backend='auto',
          evaluation_strategy=None,
          push_to_hub_model_id=None,
          push_to_hub_organization=None,
          push_to_hub_token=None,
          mp_parameters='',
          auto_find_batch_size=False,
          full_determinism=False,
          torchdynamo=None,
          ray_scope='last',
          ddp_timeout=1800,
          torch_compile=True,
          torch_compile_backend='inductor',
          torch_compile_mode=None,
          dispatch_batches=False,
          split_batches=None,
          include_tokens_per_second=True,
          include_num_input_tokens_seen=True,
          neftune_noise_alpha=None,
          optim_target_modules=None,
          batch_eval_metrics=False,
          eval_on_start=False,
          use_liger_kernel=False,
          eval_use_gather_object=False,
          average_tokens_across_devices=False,
          dataset_text_field='text',
          packing=False,
          max_seq_length=2048,
          dataset_num_proc=None,
          dataset_batch_size=1000,
          model_init_kwargs=None,
          dataset_kwargs=None,
          eval_packing=None,
          num_of_sequences=1024,
          chars_per_token=3.6,
          use_liger=False)
[2025-02-13 15:12:38,736][oumi][rank0][pid:1122761][MainThread][INFO]][device_utils.py:297] GPU Metrics Before Training: GPU runtime info: NVidiaGpuRuntimeInfo(device_index=0, device_count=4, used_memory_mb=4034.0, temperature=44, fan_speed=None, fan_speeds=None, power_usage_watts=76.105, power_limit_watts=550.0, gpu_utilization=28, memory_utilization=1, performance_state=0, clock_speed_graphics=1245, clock_speed_sm=1245, clock_speed_memory=1593).
[2025-02-13 15:12:38,736][oumi][rank0][pid:1122761][MainThread][INFO]][train.py:326] Training init time: 691.540s
[2025-02-13 15:12:38,736][oumi][rank0][pid:1122761][MainThread][INFO]][train.py:327] Starting training... (TrainerType.TRL_SFT, transformers: 4.48.3)
